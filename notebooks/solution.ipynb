{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MaLSTM](https://dl.acm.org/citation.cfm?id=3016291)を使った短いテキストの距離学習。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from time import time\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はGoogleNewsのword2vecの学習済モデルを使うためダウンロードし解凍しておく。\n",
    "```bash\n",
    "wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "gunzip GoogleNews-vectors-negative300.bin.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "DATADIR = Path('input')\n",
    "MODELDIR = Path('models')\n",
    "\n",
    "TRAIN_CSV = DATADIR / 'train.csv'\n",
    "TEST_CSV = DATADIR / 'test.csv'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "test = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストの埋め込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:25: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [train, test]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions as word to question as number representation\n",
    "            dataset.set_value(index, question, q2n)\n",
    "            \n",
    "embedding_dim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Valデータ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max(train.question1.map(lambda x: len(x)).max(),\n",
    "                     train.question2.map(lambda x: len(x)).max(),\n",
    "                     test.question1.map(lambda x: len(x)).max(),\n",
    "                     test.question2.map(lambda x: len(x)).max())\n",
    "\n",
    "# Split to train validation\n",
    "validation_size = 40000\n",
    "training_size = len(train) - validation_size\n",
    "\n",
    "X = train[questions_cols]\n",
    "Y = train['is_duplicate']\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "X_test = {'left': test.question1, 'right': test.question2}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaLSTMモデルの構築\n",
    "- 2対のインプットをEmbedding\n",
    "- Embeddingされたベクトルを共通のLSTMへの入力となる\n",
    "- LSTMは50次元のベクトルを出力し、右と左のベクトル同士の負の指数マンハッタン距離を出す\n",
    "- 類似度は0〜1で計算されているので、正解ラベルとのMSEを小さくなるように学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaLSTM(input_size: np.ndarray, embedding_matrix: np.ndarray) -> keras.models.Model:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_size (int): input size\n",
    "        embedding_matrix (np.ndarray): A ndarray shape of (XXX, XXX).\n",
    "    Returns:\n",
    "        model (keras.models.Model): built and compiled keras model object.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_input_layer(input_size):\n",
    "        \"\"\"build input layer\"\"\"\n",
    "        return Input(shape=input_size)\n",
    "    \n",
    "    \n",
    "    def build_embedding_layer(input_dim, output_dim,\n",
    "                              weights, input_length, trainable=False):\n",
    "        \"\"\"build embedding layer\"\"\"\n",
    "        return Embedding(input_dim=input_dim, output_dim=output_dim, weights=[embeddings], input_length=input_length, trainable=False)\n",
    "    \n",
    "    \n",
    "    def build_lstm_layer(n_hidden=50):\n",
    "        \"\"\"build lstm layer\"\"\"\n",
    "        return LSTM(units=n_hidden)\n",
    "        \n",
    "\n",
    "    def exponent_neg_manhattan_distance(left, right):\n",
    "        ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "        return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "    left_input = build_input_layer(input_size)\n",
    "    right_input =  build_input_layer(input_size)\n",
    "\n",
    "    embedding_layer = build_embedding_layer(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=input_size, trainable=False)\n",
    "    encoded_left = embedding_layer(left_input)\n",
    "    encoded_right = embedding_layer(right_input)\n",
    "\n",
    "    # Since this is a siamese network, both sides share the same LSTM\n",
    "    shared_lstm = build_lstm_layer(n_hidden=50)\n",
    "    left_output = shared_lstm(encoded_left)\n",
    "    right_output = shared_lstm(encoded_right)\n",
    "\n",
    "    # Calculates the distance as defined by the MaLSTM model\n",
    "    malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "    \n",
    "    model = Model([left_input, right_input], [malstm_distance])\n",
    "    optimizer = Adadelta(clipnorm=1.25)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_callbacks(save_root):\n",
    "    if not os.path.exists(save_root):\n",
    "        os.makedirs(save_root)\n",
    "    tensorboard_dir = os.path.join(save_root, 'tensorboard')\n",
    "    checkpoint_path = os.path.join(save_root, 'weights.{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.hdf5')\n",
    "    csv_path = os.path.join(save_root, 'log.csv')\n",
    "    # TensorBoard\n",
    "    tensorboard = TensorBoard(log_dir=tensorboard_dir)\n",
    "    # エポックごとの自動セーブ\n",
    "    model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1)\n",
    "    # csv結果出力\n",
    "    csv_logger = CSVLogger(filename=csv_path)\n",
    "\n",
    "    callbacks = [tensorboard, model_checkpoint, csv_logger]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 364290 samples, validate on 40000 samples\n",
      "Epoch 1/25\n",
      "364290/364290 [==============================] - 1253s 3ms/step - loss: 0.1740 - acc: 0.7459 - val_loss: 0.1585 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00001: saving model to ./weights.01-0.1585-0.7732.hdf5\n",
      "Epoch 2/25\n",
      " 73344/364290 [=====>........................] - ETA: 15:49 - loss: 0.1557 - acc: 0.7811"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318848/364290 [=========================>....] - ETA: 2:28 - loss: 0.1540 - acc: 0.7829"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206528/364290 [================>.............] - ETA: 8:33 - loss: 0.1472 - acc: 0.7942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364290/364290 [==============================] - 1238s 3ms/step - loss: 0.1460 - acc: 0.7952 - val_loss: 0.1447 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00003: saving model to ./weights.03-0.1447-0.7991.hdf5\n",
      "Epoch 4/25\n",
      " 88320/364290 [======>.......................] - ETA: 15:07 - loss: 0.1432 - acc: 0.8015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275520/364290 [=====================>........] - ETA: 4:52 - loss: 0.1416 - acc: 0.8034"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "n_epoch=25\n",
    "callbacks=get_callbacks('.')\n",
    "\n",
    "model = MaLSTM(input_size=(max_seq_length,),embedding_matrix=embeddings)\n",
    "malstm_trained = model.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, epochs=n_epoch,\n",
    "                           validation_data=([X_validation['left'], X_validation['right']], Y_validation),\n",
    "                           callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(malstm_trained.history['acc'])\n",
    "plt.plot(malstm_trained.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(malstm_trained.history['loss'])\n",
    "plt.plot(malstm_trained.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
